{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "283ce6cf234e486b9c07e36bcfa6828c",
            "0edbf3e4095a4745a8c00ac2ebf9a999",
            "b6142e1f524b4b5b9272343b87d41b62",
            "8096f85097364e7b821b1950ee5752f3",
            "9ed108d1645546cc988f35b63f684a81",
            "cdaddb290d5f41e788adaa7f22e0128f",
            "18e0f490f48e4ae0af403f87db9ab8f7",
            "ed40b324ca834ee1b5d9a1d5260c5b6b",
            "0d64b4c8af264665a0d69ef8417ec472",
            "d4eb820290e34b3bada4d81ceccd7532",
            "f1c4601aa59944c69c3072d09342c7a5",
            "69571456741d4cb0a9bbffd45dfa25cf",
            "ea789815d7fa42bcbac10424581e0d75",
            "a12eeac1fd38438b8e02550197ec6079",
            "59fccf02d34e463582105f5a13579e8e",
            "ea1fe098989f488f9c8b61ac28fe043f",
            "549da9acc6c94a8b95601c44fcbb1e8a",
            "95652cd25d494439bf5ac92a3cb98425",
            "dc84d2a4f8a540aca0992bfc481efe84",
            "19052620a0e4468aaad9cb876cb3ca46",
            "0ecb7f91fd8e4494baa8748f84f08b2c",
            "7f0fc3f9970f446988fe55997d3e98a0"
          ]
        },
        "id": "WXzcMwbmjGJY",
        "outputId": "70f0be46-42e8-4c40-fd3a-a4d5fb71df43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 30.1MB/s]                    \n",
            "2024-10-13 11:05:06 INFO: Downloaded file to C:\\Users\\ACER\\stanza_resources\\resources.json\n",
            "2024-10-13 11:05:06 INFO: Downloading default packages for language: id (Indonesian) ...\n",
            "2024-10-13 11:05:08 INFO: File exists: C:\\Users\\ACER\\stanza_resources\\id\\default.zip\n",
            "2024-10-13 11:05:12 INFO: Finished downloading models and saved to C:\\Users\\ACER\\stanza_resources\n",
            "2024-10-13 11:05:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.8MB/s]                    \n",
            "2024-10-13 11:05:12 INFO: Downloaded file to C:\\Users\\ACER\\stanza_resources\\resources.json\n",
            "2024-10-13 11:05:13 INFO: Loading these models for language: id (Indonesian):\n",
            "===============================\n",
            "| Processor    | Package      |\n",
            "-------------------------------\n",
            "| tokenize     | gsd          |\n",
            "| mwt          | gsd          |\n",
            "| pos          | gsd_charlm   |\n",
            "| lemma        | gsd_nocharlm |\n",
            "| constituency | icon_charlm  |\n",
            "| depparse     | gsd_charlm   |\n",
            "===============================\n",
            "\n",
            "2024-10-13 11:05:14 INFO: Using device: cpu\n",
            "2024-10-13 11:05:14 INFO: Loading: tokenize\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:14 INFO: Loading: mwt\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:14 INFO: Loading: pos\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:14 INFO: Loading: lemma\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:14 INFO: Loading: constituency\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\constituency\\base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:15 INFO: Loading: depparse\n",
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "2024-10-13 11:05:15 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "# Install library yang dibutuhkan\n",
        "# pip install nltk\n",
        "# pip install Sastrawi\n",
        "# pip install stanza\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean\n",
        "import re\n",
        "import nltk\n",
        "import stanza\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Download data nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Inisialisasi stopwords, stemmer, and lemmatizer\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stanza.download('id')\n",
        "nlp = stanza.Pipeline('id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "UI Dengan Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app_lsi.py\n"
          ]
        }
      ],
      "source": [
        "# Install library yang dibutuhkan\n",
        "# Buat file app.py di dalam jupyter notebook\n",
        "# eksekusi streamlit dengan aktivasi enviroment tempat streamlit diinstall (.venv\\Scripts\\python -m streamlit run app_lsi.py)\n",
        "\n",
        "%%writefile app_lsi.py\n",
        "import streamlit as st\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean\n",
        "import re\n",
        "import nltk\n",
        "import stanza\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Download data nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Inisialisasi stopwords, stemmer, and lemmatizer\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stanza.download('id')\n",
        "nlp = stanza.Pipeline('id')\n",
        "\n",
        "# Preprocessing functions\n",
        "def case_folding(text):\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "def stemming(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "def lemmatization(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    doc = nlp(text)\n",
        "    return [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "def preprocess_with_stemming(text):\n",
        "    text = case_folding(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = stemming(tokens)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_with_lemmatization(text):\n",
        "    text = case_folding(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = lemmatization(tokens)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# TF-IDF functions\n",
        "def tfidf_with_stemming(documents):\n",
        "    preprocessed_documents = [preprocess_with_stemming(doc) for doc in documents]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "def tfidf_with_lemmatization(documents):\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "# TF functions\n",
        "def tf_with_lemmatization(documents):\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tf_matrix, vectorizer\n",
        "\n",
        "def binary_tf_with_lemmatization(documents):\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    binary_tf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return binary_tf_matrix, vectorizer\n",
        "\n",
        "def log_tf_with_lemmatization(documents):\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents).toarray()\n",
        "    log_tf_matrix = np.log1p(tf_matrix)\n",
        "    return log_tf_matrix, vectorizer\n",
        "\n",
        "def augmented_tf_with_lemmatization(documents):\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents).toarray()\n",
        "    augmented_tf_matrix = 0.5 + 0.5 * (tf_matrix / tf_matrix.max(axis=1, keepdims=True))\n",
        "    return augmented_tf_matrix, vectorizer\n",
        "\n",
        "# LSI search functions\n",
        "def lsi_search(documents, query, vectorizer_func, n_components=20, similarity_metric='cosine'):\n",
        "    matrix, vectorizer = vectorizer_func(documents)\n",
        "    \n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    lsi_matrix = svd.fit_transform(matrix)\n",
        "    \n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "    \n",
        "    if similarity_metric == 'cosine':\n",
        "        similarities = cosine_similarity(query_lsi, lsi_matrix)[0]\n",
        "        ranked_docs = np.argsort(similarities)[::-1]\n",
        "        scores = similarities\n",
        "    elif similarity_metric == 'euclidean':\n",
        "        distances = [euclidean(query_lsi[0], doc_vec) for doc_vec in lsi_matrix]\n",
        "        ranked_docs = np.argsort(distances)\n",
        "        scores = distances\n",
        "    \n",
        "    return ranked_docs, scores\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"LSI-based Document Search System\")\n",
        "    \n",
        "    # Input for documents\n",
        "    documents = st.text_area(\"Enter your documents (one per line):\", height=200)\n",
        "    documents = documents.split('\\n')\n",
        "    \n",
        "    # Input for query\n",
        "    query = st.text_input(\"Enter your search query:\")\n",
        "    \n",
        "    # Select vectorization method\n",
        "    vectorization_method = st.selectbox(\n",
        "        \"Select vectorization method:\",\n",
        "        [\"TF-IDF (Lemmatization)\", \"TF (Lemmatization)\", \"Binary TF (Lemmatization)\", \"Log TF (Lemmatization)\", \"Augmented TF (Lemmatization)\"]\n",
        "    )\n",
        "    \n",
        "    # Select number of LSI components\n",
        "    n_components = st.slider(\"Number of LSI components:\", min_value=2, max_value=50, value=20)\n",
        "    \n",
        "    # Select similarity metric\n",
        "    similarity_metric = st.radio(\"Select similarity metric:\", [\"Cosine Similarity\", \"Euclidean Distance\"])\n",
        "    \n",
        "    if st.button(\"Search\"):\n",
        "        if documents and query:\n",
        "            # Choose vectorization function\n",
        "            if vectorization_method == \"TF-IDF (Lemmatization)\":\n",
        "                vectorizer_func = tfidf_with_lemmatization\n",
        "            elif vectorization_method == \"TF (Lemmatization)\":\n",
        "                vectorizer_func = tf_with_lemmatization\n",
        "            elif vectorization_method == \"Binary TF (Lemmatization)\":\n",
        "                vectorizer_func = binary_tf_with_lemmatization\n",
        "            elif vectorization_method == \"Log TF (Lemmatization)\":\n",
        "                vectorizer_func = log_tf_with_lemmatization\n",
        "            else:  # Augmented TF (Lemmatization)\n",
        "                vectorizer_func = augmented_tf_with_lemmatization\n",
        "            \n",
        "            # Perform search\n",
        "            ranked_docs, scores = lsi_search(\n",
        "                documents, \n",
        "                query, \n",
        "                vectorizer_func, \n",
        "                n_components=n_components, \n",
        "                similarity_metric='cosine' if similarity_metric == \"Cosine Similarity\" else 'euclidean'\n",
        "            )\n",
        "            \n",
        "            # Display results\n",
        "            st.subheader(\"Search Results:\")\n",
        "            for i, idx in enumerate(ranked_docs[:10]):  # Display top 10 results\n",
        "                score = scores[idx]\n",
        "                if similarity_metric == \"Cosine Similarity\":\n",
        "                    st.write(f\"{i+1}. (Similarity: {score:.4f}) {documents[idx]}\")\n",
        "                else:\n",
        "                    st.write(f\"{i+1}. (Euclidean Distance: {score:.4f}) {documents[idx]}\")\n",
        "        else:\n",
        "            st.warning(\"Please enter both documents and a search query.\")\n",
        "        \n",
        "        # Preprocessing demonstration\n",
        "    if st.checkbox(\"Show Preprocessing Results\"):\n",
        "        st.subheader(\"Preprocessing Results:\")\n",
        "        for doc in documents:\n",
        "            original_text = case_folding(doc)\n",
        "            tokens = tokenize(original_text)\n",
        "            tokens_no_stopwords = remove_stopwords(tokens)\n",
        "            stemmed_result = stemming(tokens_no_stopwords)\n",
        "            lemmatized_result = lemmatization(tokens_no_stopwords)\n",
        "            \n",
        "            st.write(f\"Original: {original_text}\")\n",
        "            st.write(f\"Stemming: {' '.join(stemmed_result)}\")\n",
        "            st.write(f\"Lemmatization: {' '.join(lemmatized_result)}\")\n",
        "            st.write(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDIO57yjb1R"
      },
      "source": [
        "**LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sSnTgRFRjfiG"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Teknologi semakin berkembang dan digunakan dalam banyak bidang.\",\n",
        "    \"Perangkat pintar membantu kita menghemat energi di rumah.\",\n",
        "    \"Pengolahan data membantu perusahaan memahami pelanggan dengan lebih baik.\",\n",
        "    \"Sistem otomatis membuat pekerjaan di pabrik menjadi lebih cepat dan efisien.\",\n",
        "    \"Kecerdasan buatan digunakan untuk meningkatkan pelayanan kepada pelanggan.\",\n",
        "    \"Teknologi buku besar memberikan keamanan dalam transaksi online.\",\n",
        "    \"Jaringan perangkat membantu kita mengelola sumber daya dengan lebih mudah.\",\n",
        "    \"Realitas virtual dan realitas tertambah digunakan dalam belajar.\",\n",
        "    \"Komputasi awan memudahkan akses data di mana saja.\",\n",
        "    \"Sistem keamanan melindungi data dari ancaman yang berbahaya.\",\n",
        "    \"Pertanian organik semakin populer di kalangan petani muda.\",\n",
        "    \"Teknologi pertanian modern membantu meningkatkan hasil panen.\",\n",
        "    \"Pupuk organik dapat meningkatkan kesuburan tanah secara alami.\",\n",
        "    \"Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\",\n",
        "    \"Penggunaan pestisida harus dilakukan dengan hati-hati untuk menghindari dampak lingkungan.\",\n",
        "    \"Program subsidi pemerintah mendukung petani kecil untuk mengembangkan usaha mereka.\",\n",
        "    \"Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\",\n",
        "    \"Diversifikasi tanaman penting untuk mengurangi risiko gagal panen.\",\n",
        "    \"Irigasi yang efisien sangat penting dalam pertanian di daerah kering.\",\n",
        "    \"Penyuluhan pertanian memberikan pengetahuan dan keterampilan kepada petani.\",\n",
        "    \"Pendidikan adalah kunci untuk menciptakan generasi yang cerdas dan kompeten.\",\n",
        "    \"Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\",\n",
        "    \"Teknologi informasi mempengaruhi metode pembelajaran di kelas.\",\n",
        "    \"Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\",\n",
        "    \"Kurikulum yang relevan membantu siswa mempersiapkan diri untuk masa depan.\",\n",
        "    \"Pelatihan guru berkualitas penting untuk meningkatkan standar pendidikan.\",\n",
        "    \"Beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu.\",\n",
        "    \"Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\",\n",
        "    \"Pendidikan vokasi memberikan keterampilan praktis kepada siswa.\",\n",
        "    \"Perpustakaan sekolah merupakan sumber daya penting untuk mendukung pembelajaran.\",\n",
        "    \"Penelitian di bidang kesehatan terus berkembang untuk menemukan pengobatan baru.\",\n",
        "    \"Pendidikan kesehatan masyarakat menjadi salah satu kunci mencegah penyakit.\",\n",
        "    \"Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\",\n",
        "    \"Pandemi global memicu reformasi dalam sistem pelayanan kesehatan.\",\n",
        "    \"Pemerataan fasilitas kesehatan di seluruh wilayah menjadi tantangan tersendiri.\",\n",
        "    \"Peran dokter dalam kesehatan masyarakat tidak hanya sebagai penyembuh, tetapi juga edukator.\",\n",
        "    \"Masalah gizi buruk masih menjadi tantangan besar di beberapa daerah.\",\n",
        "    \"Penyakit tidak menular seperti diabetes semakin meningkat di masyarakat urban.\",\n",
        "    \"Teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks.\",\n",
        "    \"Program vaksinasi sangat penting untuk mencegah wabah di masyarakat.\",\n",
        "    \"Pemerintahan yang transparan menjadi kunci dalam meningkatkan kepercayaan publik.\",\n",
        "    \"Desentralisasi membantu daerah untuk lebih mandiri dalam mengelola sumber daya.\",\n",
        "    \"Program e-government meningkatkan efisiensi layanan publik.\",\n",
        "    \"Perubahan undang-undang dapat berdampak pada tata kelola pemerintahan.\",\n",
        "    \"Anggaran negara harus dikelola dengan akuntabilitas yang tinggi.\",\n",
        "    \"Kerjasama antara pusat dan daerah sangat penting dalam pemerintahan.\",\n",
        "    \"Reformasi birokrasi berfokus pada peningkatan kualitas pelayanan.\",\n",
        "    \"Sistem pemilu yang jujur adalah dasar dari demokrasi yang sehat.\",\n",
        "    \"Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\",\n",
        "    \"Kebijakan luar negeri harus selaras dengan kepentingan nasional.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_IxYxoMjiT8"
      },
      "source": [
        "**PRE-PROCCESSING TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PN_hldqyjmaZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Case folding and cleaning (used in all scenarios)\n",
        "def case_folding(text):\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Step 3: Stopwords removal\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Step 4: Stemming\n",
        "def stemming(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Step 5: Lemmatization using stanza\n",
        "def lemmatization(tokens):\n",
        "    text = ' '.join(tokens)  # Gabungkan token menjadi teks untuk diproses\n",
        "    doc = nlp(text)\n",
        "    return [word.lemma for sent in doc.sentences for word in sent.words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QeaURWA8kl7X"
      },
      "outputs": [],
      "source": [
        "# Preprocessing with stemming\n",
        "def preprocess_with_stemming(text):\n",
        "    text = case_folding(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = stemming(tokens)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AkQBE4YTknSl"
      },
      "outputs": [],
      "source": [
        "# Preprocessing with lemmatization\n",
        "def preprocess_with_lemmatization(text):\n",
        "    text = case_folding(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = lemmatization(tokens)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asR3A3N6pUMT",
        "outputId": "17058619-502f-40a9-ff1d-267e5f4a3ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hasil Preprocessing dengan Stemming dan Lemmatization:\n",
            "Original\t: teknologi semakin berkembang dan digunakan dalam banyak bidang\n",
            "Stemming\t: teknologi kembang bidang\n",
            "Lemmatization\t: teknologi kembang bidang\n",
            "\n",
            "\n",
            "Original\t: perangkat pintar membantu kita menghemat energi di rumah\n",
            "Stemming\t: perangkat pintar bantu hemat energi rumah\n",
            "Lemmatization\t: perangkat pintar bantu hemat energi rumah\n",
            "\n",
            "\n",
            "Original\t: pengolahan data membantu perusahaan memahami pelanggan dengan lebih baik\n",
            "Stemming\t: olah data bantu usaha paham langgan\n",
            "Lemmatization\t: olah data bantu usaha paham langgan\n",
            "\n",
            "\n",
            "Original\t: sistem otomatis membuat pekerjaan di pabrik menjadi lebih cepat dan efisien\n",
            "Stemming\t: sistem otomatis kerja pabrik cepat efisien\n",
            "Lemmatization\t: sistem otomatis pekerja pabrik cepat efisien\n",
            "\n",
            "\n",
            "Original\t: kecerdasan buatan digunakan untuk meningkatkan pelayanan kepada pelanggan\n",
            "Stemming\t: cerdas buat tingkat layan langgan\n",
            "Lemmatization\t: cerdas buat tingkat layan langgan\n",
            "\n",
            "\n",
            "Original\t: teknologi buku besar memberikan keamanan dalam transaksi online\n",
            "Stemming\t: teknologi buku aman transaksi online\n",
            "Lemmatization\t: teknologi buku aman transaksi online\n",
            "\n",
            "\n",
            "Original\t: jaringan perangkat membantu kita mengelola sumber daya dengan lebih mudah\n",
            "Stemming\t: jaring perangkat bantu kelola sumber daya mudah\n",
            "Lemmatization\t: jaring perangkat bantu kelola sumber daya mudah\n",
            "\n",
            "\n",
            "Original\t: realitas virtual dan realitas tertambah digunakan dalam belajar\n",
            "Stemming\t: realitas virtual realitas tambah ajar\n",
            "Lemmatization\t: realitas virtual realitas tambah belajar\n",
            "\n",
            "\n",
            "Original\t: komputasi awan memudahkan akses data di mana saja\n",
            "Stemming\t: komputasi awan mudah akses data\n",
            "Lemmatization\t: komputasi awan mudah akses data\n",
            "\n",
            "\n",
            "Original\t: sistem keamanan melindungi data dari ancaman yang berbahaya\n",
            "Stemming\t: sistem aman lindung data ancam bahaya\n",
            "Lemmatization\t: sistem aman lindung data ancam bahaya\n",
            "\n",
            "\n",
            "Original\t: pertanian organik semakin populer di kalangan petani muda\n",
            "Stemming\t: tani organik populer kalang tani muda\n",
            "Lemmatization\t: tani organik populer kalangan petani muda\n",
            "\n",
            "\n",
            "Original\t: teknologi pertanian modern membantu meningkatkan hasil panen\n",
            "Stemming\t: teknologi tani modern bantu tingkat hasil panen\n",
            "Lemmatization\t: teknologi tani modern bantu tingkat hasil panen\n",
            "\n",
            "\n",
            "Original\t: pupuk organik dapat meningkatkan kesuburan tanah secara alami\n",
            "Stemming\t: pupuk organik tingkat subur tanah alami\n",
            "Lemmatization\t: pupuk organik tingkat subur tanah alami\n",
            "\n",
            "\n",
            "Original\t: tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik\n",
            "Stemming\t: tanam padi air tumbuh\n",
            "Lemmatization\t: tanam padi air tumbuh\n",
            "\n",
            "\n",
            "Original\t: penggunaan pestisida harus dilakukan dengan hatihati untuk menghindari dampak lingkungan\n",
            "Stemming\t: guna pestisida hatihati hindar dampak lingkung\n",
            "Lemmatization\t: guna pestisida hatihati hindar dampak lingkung\n",
            "\n",
            "\n",
            "Original\t: program subsidi pemerintah mendukung petani kecil untuk mengembangkan usaha mereka\n",
            "Stemming\t: program subsidi perintah dukung tani kembang usaha\n",
            "Lemmatization\t: program subsidi perintah dukung petani kembang usaha\n",
            "\n",
            "\n",
            "Original\t: budidaya tanaman hortikultura memberikan peluang ekonomi yang baik\n",
            "Stemming\t: budidaya tanam hortikultura peluang ekonomi\n",
            "Lemmatization\t: budidaya tanam hortikultura peluang ekonomi\n",
            "\n",
            "\n",
            "Original\t: diversifikasi tanaman penting untuk mengurangi risiko gagal panen\n",
            "Stemming\t: diversifikasi tanam kurang risiko gagal panen\n",
            "Lemmatization\t: diversifikasi tanam kurang risiko gagal panen\n",
            "\n",
            "\n",
            "Original\t: irigasi yang efisien sangat penting dalam pertanian di daerah kering\n",
            "Stemming\t: irigasi efisien tani daerah kering\n",
            "Lemmatization\t: irigasi efisien tani daerah kering\n",
            "\n",
            "\n",
            "Original\t: penyuluhan pertanian memberikan pengetahuan dan keterampilan kepada petani\n",
            "Stemming\t: suluh tani tahu terampil tani\n",
            "Lemmatization\t: suluh tani pengetahuan keterampilan petani\n",
            "\n",
            "\n",
            "Original\t: pendidikan adalah kunci untuk menciptakan generasi yang cerdas dan kompeten\n",
            "Stemming\t: didik kunci cipta generasi cerdas kompeten\n",
            "Lemmatization\t: didik kunci cipta generasi cerdas kompeten\n",
            "\n",
            "\n",
            "Original\t: sekolah dasar memiliki peranan penting dalam pengembangan karakter anak\n",
            "Stemming\t: sekolah dasar milik peran kembang karakter anak\n",
            "Lemmatization\t: sekolah dasar milik peran kembang karakter anak\n",
            "\n",
            "\n",
            "Original\t: teknologi informasi mempengaruhi metode pembelajaran di kelas\n",
            "Stemming\t: teknologi informasi pengaruh metode ajar kelas\n",
            "Lemmatization\t: teknologi informasi pengaruh metode belajar kelas\n",
            "\n",
            "\n",
            "Original\t: pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar\n",
            "Stemming\t: didik inklusif sempat anak ajar\n",
            "Lemmatization\t: didik inklusif kesempatan anak belajar\n",
            "\n",
            "\n",
            "Original\t: kurikulum yang relevan membantu siswa mempersiapkan diri untuk masa depan\n",
            "Stemming\t: kurikulum relevan bantu siswa\n",
            "Lemmatization\t: kurikulum relevan bantu siswa\n",
            "\n",
            "\n",
            "Original\t: pelatihan guru berkualitas penting untuk meningkatkan standar pendidikan\n",
            "Stemming\t: latih guru kualitas tingkat standar didik\n",
            "Lemmatization\t: latih guru kualitas tingkat standar didik\n",
            "\n",
            "\n",
            "Original\t: beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu\n",
            "Stemming\t: beasiswa didik dukung siswa prestasi keluarga\n",
            "Lemmatization\t: beasiswa didik dukung siswa prestasi keluarga\n",
            "\n",
            "\n",
            "Original\t: pembelajaran jarak jauh menjadi alternatif di masa pandemi\n",
            "Stemming\t: ajar jarak alternatif pandemi\n",
            "Lemmatization\t: belajar jarak alternatif pandemi\n",
            "\n",
            "\n",
            "Original\t: pendidikan vokasi memberikan keterampilan praktis kepada siswa\n",
            "Stemming\t: didik vokasi terampil praktis siswa\n",
            "Lemmatization\t: didik vokasi keterampilan praktis siswa\n",
            "\n",
            "\n",
            "Original\t: perpustakaan sekolah merupakan sumber daya penting untuk mendukung pembelajaran\n",
            "Stemming\t: pustaka sekolah sumber daya dukung ajar\n",
            "Lemmatization\t: perpustakaan sekolah sumber daya dukung belajar\n",
            "\n",
            "\n",
            "Original\t: penelitian di bidang kesehatan terus berkembang untuk menemukan pengobatan baru\n",
            "Stemming\t: teliti bidang sehat kembang temu obat\n",
            "Lemmatization\t: teliti bidang sehat kembang temu obat\n",
            "\n",
            "\n",
            "Original\t: pendidikan kesehatan masyarakat menjadi salah satu kunci mencegah penyakit\n",
            "Stemming\t: didik sehat masyarakat salah kunci cegah sakit\n",
            "Lemmatization\t: didik sehat masyarakat salah kunci cegah sakit\n",
            "\n",
            "\n",
            "Original\t: asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak\n",
            "Stemming\t: asupan nutrisi imbang tumbuh anak\n",
            "Lemmatization\t: asup nutrisi seimbang tumbuh anak\n",
            "\n",
            "\n",
            "Original\t: pandemi global memicu reformasi dalam sistem pelayanan kesehatan\n",
            "Stemming\t: pandemi global picu reformasi sistem layan sehat\n",
            "Lemmatization\t: pandemi global picu reformasi sistem layan sehat\n",
            "\n",
            "\n",
            "Original\t: pemerataan fasilitas kesehatan di seluruh wilayah menjadi tantangan tersendiri\n",
            "Stemming\t: perata fasilitas sehat wilayah tantang sendiri\n",
            "Lemmatization\t: perata fasilitas sehat wilayah tantang sendiri\n",
            "\n",
            "\n",
            "Original\t: peran dokter dalam kesehatan masyarakat tidak hanya sebagai penyembuh tetapi juga edukator\n",
            "Stemming\t: peran dokter sehat masyarakat sembuh edukator\n",
            "Lemmatization\t: peran dokter sehat masyarakat sembuh edukator\n",
            "\n",
            "\n",
            "Original\t: masalah gizi buruk masih menjadi tantangan besar di beberapa daerah\n",
            "Stemming\t: gizi buruk tantang daerah\n",
            "Lemmatization\t: gizi buruk tantang daerah\n",
            "\n",
            "\n",
            "Original\t: penyakit tidak menular seperti diabetes semakin meningkat di masyarakat urban\n",
            "Stemming\t: sakit tular diabetes tingkat masyarakat urban\n",
            "Lemmatization\t: sakit tular diabetes tingkat masyarakat urban\n",
            "\n",
            "\n",
            "Original\t: teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks\n",
            "Stemming\t: teknologi medis canggih bantu diagnosis awat sakit kompleks\n",
            "Lemmatization\t: teknologi medis canggih bantu diagnosis rawat sakit kompleks\n",
            "\n",
            "\n",
            "Original\t: program vaksinasi sangat penting untuk mencegah wabah di masyarakat\n",
            "Stemming\t: program vaksinasi cegah wabah masyarakat\n",
            "Lemmatization\t: program vaksinasi cegah wabah masyarakat\n",
            "\n",
            "\n",
            "Original\t: pemerintahan yang transparan menjadi kunci dalam meningkatkan kepercayaan publik\n",
            "Stemming\t: perintah transparan kunci tingkat percaya publik\n",
            "Lemmatization\t: perintah transparan kunci tingkat percaya publik\n",
            "\n",
            "\n",
            "Original\t: desentralisasi membantu daerah untuk lebih mandiri dalam mengelola sumber daya\n",
            "Stemming\t: desentralisasi bantu daerah mandiri kelola sumber daya\n",
            "Lemmatization\t: desentralisasi bantu daerah mandiri kelola sumber daya\n",
            "\n",
            "\n",
            "Original\t: program egovernment meningkatkan efisiensi layanan publik\n",
            "Stemming\t: program egovernment tingkat efisiensi layan publik\n",
            "Lemmatization\t: program egovernment tingkat efisiensi layan publik\n",
            "\n",
            "\n",
            "Original\t: perubahan undangundang dapat berdampak pada tata kelola pemerintahan\n",
            "Stemming\t: ubah undangundang dampak tata kelola perintah\n",
            "Lemmatization\t: ubah undangundang dampak tata kelola perintah\n",
            "\n",
            "\n",
            "Original\t: anggaran negara harus dikelola dengan akuntabilitas yang tinggi\n",
            "Stemming\t: anggar negara kelola akuntabilitas\n",
            "Lemmatization\t: anggar negara kelola akuntabilitas\n",
            "\n",
            "\n",
            "Original\t: kerjasama antara pusat dan daerah sangat penting dalam pemerintahan\n",
            "Stemming\t: kerjasama pusat daerah perintah\n",
            "Lemmatization\t: kerjasama pusat daerah perintah\n",
            "\n",
            "\n",
            "Original\t: reformasi birokrasi berfokus pada peningkatan kualitas pelayanan\n",
            "Stemming\t: reformasi birokrasi fokus tingkat kualitas layan\n",
            "Lemmatization\t: reformasi birokrasi fokus tingkat kualitas layan\n",
            "\n",
            "\n",
            "Original\t: sistem pemilu yang jujur adalah dasar dari demokrasi yang sehat\n",
            "Stemming\t: sistem milu jujur dasar demokrasi sehat\n",
            "Lemmatization\t: sistem pemilu jujur dasar demokrasi sehat\n",
            "\n",
            "\n",
            "Original\t: peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan\n",
            "Stemming\t: peran aktif masyarakat awas perintah butuh\n",
            "Lemmatization\t: peran aktif masyarakat awas perintah butuh\n",
            "\n",
            "\n",
            "Original\t: kebijakan luar negeri harus selaras dengan kepentingan nasional\n",
            "Stemming\t: bijak negeri selaras penting nasional\n",
            "Lemmatization\t: bijak negeri selaras penting nasional\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Menerapkan preprocessing pada setiap dokumen\n",
        "print(\"Hasil Preprocessing dengan Stemming dan Lemmatization:\")\n",
        "for doc in documents:\n",
        "    # Proses case folding\n",
        "    original_text = case_folding(doc)\n",
        "\n",
        "    # Tokenisasi\n",
        "    tokens = tokenize(original_text)\n",
        "\n",
        "    # Stopwords removal\n",
        "    tokens_no_stopwords = remove_stopwords(tokens)\n",
        "\n",
        "    # Proses stemming\n",
        "    stemmed_result = stemming(tokens_no_stopwords)\n",
        "\n",
        "    # Proses lemmatization\n",
        "    lemmatized_result = lemmatization(tokens_no_stopwords)\n",
        "\n",
        "    # Menampilkan hasil\n",
        "    print(f\"Original\\t: {original_text}\")\n",
        "    print(f\"Stemming\\t: {' '.join(stemmed_result)}\")\n",
        "    print(f\"Lemmatization\\t: {' '.join(lemmatized_result)}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StoUSalcktDm"
      },
      "source": [
        "**PEMBOBOTAN KATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Agd2d7HzkvXV"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk membuat TF-IDF dengan stemming\n",
        "def tfidf_with_stemming(documents):\n",
        "    # Preprocessing dengan stemming\n",
        "    preprocessed_documents = [preprocess_with_stemming(doc) for doc in documents]\n",
        "    # Buat TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tfidf_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y3JS_EQ2kxYv"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk membuat TF-IDF dengan lemmatization\n",
        "def tfidf_with_lemmatization(documents):\n",
        "    # Preprocessing dengan lemmatization\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    # Buat TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tfidf_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdJs2iXJk0iy"
      },
      "source": [
        "**QUERY PENCARIAN DOKUMEN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BQoi7pPTltGe"
      },
      "outputs": [],
      "source": [
        "query = \"Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0kSQwOMoS36"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoq8hUkpm5-D",
        "outputId": "a2676880-fd60-4348-fb78-616e98739b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (Stemming):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 29 \t (Similarity: 0.9999) \t: Pendidikan vokasi memberikan keterampilan praktis kepada siswa.\n",
            "2. Doc 24 \t (Similarity: 0.9998) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "3. Doc 22 \t (Similarity: 0.9902) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "4. Doc 27 \t (Similarity: 0.9813) \t: Beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu.\n",
            "5. Doc 28 \t (Similarity: 0.9741) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "6. Doc 1 \t (Similarity: 0.9683) \t: Teknologi semakin berkembang dan digunakan dalam banyak bidang.\n",
            "7. Doc 31 \t (Similarity: 0.9683) \t: Penelitian di bidang kesehatan terus berkembang untuk menemukan pengobatan baru.\n",
            "8. Doc 10 \t (Similarity: 0.9461) \t: Sistem keamanan melindungi data dari ancaman yang berbahaya.\n",
            "9. Doc 33 \t (Similarity: 0.9401) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "10. Doc 16 \t (Similarity: 0.9341) \t: Program subsidi pemerintah mendukung petani kecil untuk mengembangkan usaha mereka.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan stemming\n",
        "def lsi_with_stemming(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan stemming\n",
        "    tfidf_matrix, vectorizer = tfidf_with_stemming(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=2, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan stemming\n",
        "    preprocessed_query = preprocess_with_stemming(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (Stemming):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan stemming\n",
        "lsi_with_stemming(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB2L-dKxoXWG"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzr9dAXaoZxo",
        "outputId": "3fea8a1a-f24e-4083-fce4-957fdbbcb0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (Lemmatization):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 24 \t (Similarity: 0.9987) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "2. Doc 16 \t (Similarity: 0.9984) \t: Program subsidi pemerintah mendukung petani kecil untuk mengembangkan usaha mereka.\n",
            "3. Doc 22 \t (Similarity: 0.9983) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "4. Doc 1 \t (Similarity: 0.9976) \t: Teknologi semakin berkembang dan digunakan dalam banyak bidang.\n",
            "5. Doc 29 \t (Similarity: 0.9947) \t: Pendidikan vokasi memberikan keterampilan praktis kepada siswa.\n",
            "6. Doc 27 \t (Similarity: 0.9910) \t: Beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu.\n",
            "7. Doc 4 \t (Similarity: 0.9669) \t: Sistem otomatis membuat pekerjaan di pabrik menjadi lebih cepat dan efisien.\n",
            "8. Doc 28 \t (Similarity: 0.9606) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "9. Doc 10 \t (Similarity: 0.9566) \t: Sistem keamanan melindungi data dari ancaman yang berbahaya.\n",
            "10. Doc 12 \t (Similarity: 0.9550) \t: Teknologi pertanian modern membantu meningkatkan hasil panen.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=2, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (Lemmatization):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er-v2wKxosQd"
      },
      "source": [
        "**Kesimpulan**\n",
        "\n",
        "\n",
        "> Pra-pemrosesan dokumen menggunakan lemmatization menghasilkan nilai similarity yang lebih tinggi dibandingkan menggunakan stemming. Oleh karena itu lemmatization akan digunakan untuk skenario selanjutnya.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23bDUsl4xdJf"
      },
      "source": [
        "# SKENARIO 2: MENGUBAH JUMLAH TOPIK LATEN (N_COMPONENTS)\n",
        "\n",
        "> Mesin pencarian belum menghasilkan pencarian yang akurat berdasarkan query yang diberikan, oleh karena itu dilakukan skenario untuk mengubah jumlah topik laten [2, 5, 10, 20].\n",
        "\n",
        "\n",
        "> Hipotesis: Menggunakan lebih banyak topik laten akan menghasilkan pencarian yang lebih akurat, tetapi setelah titik tertentu, penambahan topik laten mungkin tidak lagi memberikan peningkatan yang signifikan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DKPluVwx3Iu"
      },
      "source": [
        "**n_components=2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_79ML4xwuG",
        "outputId": "a0f877d7-0cce-436b-913e-0eb5cc2b7290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (n_components=2):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 24 \t (Similarity: 0.9987) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "2. Doc 16 \t (Similarity: 0.9984) \t: Program subsidi pemerintah mendukung petani kecil untuk mengembangkan usaha mereka.\n",
            "3. Doc 22 \t (Similarity: 0.9983) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "4. Doc 1 \t (Similarity: 0.9976) \t: Teknologi semakin berkembang dan digunakan dalam banyak bidang.\n",
            "5. Doc 29 \t (Similarity: 0.9947) \t: Pendidikan vokasi memberikan keterampilan praktis kepada siswa.\n",
            "6. Doc 27 \t (Similarity: 0.9910) \t: Beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu.\n",
            "7. Doc 4 \t (Similarity: 0.9669) \t: Sistem otomatis membuat pekerjaan di pabrik menjadi lebih cepat dan efisien.\n",
            "8. Doc 28 \t (Similarity: 0.9606) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "9. Doc 10 \t (Similarity: 0.9566) \t: Sistem keamanan melindungi data dari ancaman yang berbahaya.\n",
            "10. Doc 12 \t (Similarity: 0.9550) \t: Teknologi pertanian modern membantu meningkatkan hasil panen.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=2, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (n_components=2):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJrNCKDx9-M"
      },
      "source": [
        "**n_components=5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUX9lIIAx9GN",
        "outputId": "6e4ba532-fb58-42ef-c8cf-5b20bfd4ffb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (n_components=5):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9387) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 22 \t (Similarity: 0.8562) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "3. Doc 24 \t (Similarity: 0.8147) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "4. Doc 28 \t (Similarity: 0.7833) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "5. Doc 8 \t (Similarity: 0.7737) \t: Realitas virtual dan realitas tertambah digunakan dalam belajar.\n",
            "6. Doc 14 \t (Similarity: 0.6496) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "7. Doc 32 \t (Similarity: 0.6169) \t: Pendidikan kesehatan masyarakat menjadi salah satu kunci mencegah penyakit.\n",
            "8. Doc 27 \t (Similarity: 0.6072) \t: Beasiswa pendidikan mendukung siswa berprestasi dari keluarga kurang mampu.\n",
            "9. Doc 31 \t (Similarity: 0.6023) \t: Penelitian di bidang kesehatan terus berkembang untuk menemukan pengobatan baru.\n",
            "10. Doc 23 \t (Similarity: 0.5795) \t: Teknologi informasi mempengaruhi metode pembelajaran di kelas.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=5, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (n_components=5):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP8fppWlx_g6"
      },
      "source": [
        "**n_components=10**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEM7vjduyDnJ",
        "outputId": "9aa81486-4d0f-4426-babf-1aa15c9c1940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (n_components=10):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9893) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 14 \t (Similarity: 0.9148) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "3. Doc 17 \t (Similarity: 0.8919) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "4. Doc 18 \t (Similarity: 0.7168) \t: Diversifikasi tanaman penting untuk mengurangi risiko gagal panen.\n",
            "5. Doc 22 \t (Similarity: 0.6314) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "6. Doc 50 \t (Similarity: 0.5214) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "7. Doc 24 \t (Similarity: 0.4424) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "8. Doc 49 \t (Similarity: 0.3240) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "9. Doc 37 \t (Similarity: 0.2706) \t: Masalah gizi buruk masih menjadi tantangan besar di beberapa daerah.\n",
            "10. Doc 46 \t (Similarity: 0.2563) \t: Kerjasama antara pusat dan daerah sangat penting dalam pemerintahan.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=10, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (n_components=10):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXi12KnVyAyL"
      },
      "source": [
        "**n_components=20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXPT0FTJyGmb",
        "outputId": "98c88bd9-d762-4cc9-c3c0-676a202ea281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (n_components=20):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9815) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 14 \t (Similarity: 0.6094) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "3. Doc 24 \t (Similarity: 0.5476) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "4. Doc 22 \t (Similarity: 0.5101) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "5. Doc 49 \t (Similarity: 0.1544) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "6. Doc 2 \t (Similarity: 0.0917) \t: Perangkat pintar membantu kita menghemat energi di rumah.\n",
            "7. Doc 36 \t (Similarity: 0.0884) \t: Peran dokter dalam kesehatan masyarakat tidak hanya sebagai penyembuh, tetapi juga edukator.\n",
            "8. Doc 45 \t (Similarity: 0.0692) \t: Anggaran negara harus dikelola dengan akuntabilitas yang tinggi.\n",
            "9. Doc 46 \t (Similarity: 0.0648) \t: Kerjasama antara pusat dan daerah sangat penting dalam pemerintahan.\n",
            "10. Doc 44 \t (Similarity: 0.0636) \t: Perubahan undang-undang dapat berdampak pada tata kelola pemerintahan.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (n_components=20):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExNI2SlqzExu"
      },
      "source": [
        "**Kesimpulan**\n",
        "\n",
        "\n",
        "> Mesin pencarian bekerja dengan baik saat nilai n_components=10 (analisis lebih lanjut)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKL2hvck5zb6"
      },
      "source": [
        "# SKENARIO 3: VARIASI PEMBOBOTAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbtCrD5m61bk"
      },
      "source": [
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdEVdMWS63wN",
        "outputId": "9dcca7c4-00ac-46f8-bec2-b3fa2de26a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (TF-IDF):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9815) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 14 \t (Similarity: 0.6094) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "3. Doc 24 \t (Similarity: 0.5476) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "4. Doc 22 \t (Similarity: 0.5101) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "5. Doc 49 \t (Similarity: 0.1544) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "6. Doc 2 \t (Similarity: 0.0917) \t: Perangkat pintar membantu kita menghemat energi di rumah.\n",
            "7. Doc 36 \t (Similarity: 0.0884) \t: Peran dokter dalam kesehatan masyarakat tidak hanya sebagai penyembuh, tetapi juga edukator.\n",
            "8. Doc 45 \t (Similarity: 0.0692) \t: Anggaran negara harus dikelola dengan akuntabilitas yang tinggi.\n",
            "9. Doc 46 \t (Similarity: 0.0648) \t: Kerjasama antara pusat dan daerah sangat penting dalam pemerintahan.\n",
            "10. Doc 44 \t (Similarity: 0.0636) \t: Perubahan undang-undang dapat berdampak pada tata kelola pemerintahan.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan lemmatization\n",
        "def lsi_with_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tfidf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (TF-IDF):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization\n",
        "lsi_with_lemmatization(documents, query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWHj7FXz66UM"
      },
      "source": [
        "**Term Frequency (TF) Biasa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdpozrUE7K8M",
        "outputId": "72a98889-c3d8-441e-c3e3-79f87a30c40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (TF Biasa):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9833) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 22 \t (Similarity: 0.5951) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "3. Doc 14 \t (Similarity: 0.5935) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "4. Doc 24 \t (Similarity: 0.5472) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "5. Doc 50 \t (Similarity: 0.4201) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "6. Doc 17 \t (Similarity: 0.2642) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "7. Doc 49 \t (Similarity: 0.2029) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "8. Doc 39 \t (Similarity: 0.1394) \t: Teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks.\n",
            "9. Doc 28 \t (Similarity: 0.1316) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "10. Doc 5 \t (Similarity: 0.1137) \t: Kecerdasan buatan digunakan untuk meningkatkan pelayanan kepada pelanggan.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung TF biasa\n",
        "def tf_with_lemmatization(documents):\n",
        "    # Preprocessing dengan lemmatization\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    # Buat CountVectorizer untuk menghitung TF biasa\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return tf_matrix, vectorizer\n",
        "\n",
        "# Fungsi LSI dengan TF Biasa\n",
        "def lsi_with_tf_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF biasa dengan lemmatization\n",
        "    tf_matrix, vectorizer = tf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (TF Biasa):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian\n",
        "lsi_with_tf_lemmatization(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEF73cq7C26"
      },
      "source": [
        "**Binary Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su6aSQ517Pfx",
        "outputId": "d2603be1-98c6-4357-c382-1a73e7063a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (Binary TF):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9827) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 14 \t (Similarity: 0.5926) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "3. Doc 22 \t (Similarity: 0.5882) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "4. Doc 24 \t (Similarity: 0.5422) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "5. Doc 50 \t (Similarity: 0.4290) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "6. Doc 17 \t (Similarity: 0.2649) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "7. Doc 49 \t (Similarity: 0.1928) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "8. Doc 39 \t (Similarity: 0.1605) \t: Teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks.\n",
            "9. Doc 28 \t (Similarity: 0.1134) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "10. Doc 8 \t (Similarity: 0.1105) \t: Realitas virtual dan realitas tertambah digunakan dalam belajar.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung Binary TF\n",
        "def binary_tf_with_lemmatization(documents):\n",
        "    # Preprocessing dengan lemmatization\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    # Buat CountVectorizer dengan binary=True untuk binary TF\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    binary_tf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "    return binary_tf_matrix, vectorizer\n",
        "\n",
        "# Fungsi LSI dengan Binary TF\n",
        "def lsi_with_binary_tf_lemmatization(documents, query, random_state=42):\n",
        "    # Buat binary TF dengan lemmatization\n",
        "    tf_matrix, vectorizer = binary_tf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (Binary TF):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian\n",
        "lsi_with_binary_tf_lemmatization(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSyZFwmM7FcA"
      },
      "source": [
        "**Logarithmic Normalization Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YGEnfKJ7VEr",
        "outputId": "46894c6c-9f86-43bd-c80f-998514cb28a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (Log TF):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.9829) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 22 \t (Similarity: 0.5987) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "3. Doc 14 \t (Similarity: 0.5938) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "4. Doc 24 \t (Similarity: 0.5303) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "5. Doc 50 \t (Similarity: 0.3773) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "6. Doc 17 \t (Similarity: 0.2686) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "7. Doc 49 \t (Similarity: 0.1971) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "8. Doc 39 \t (Similarity: 0.1573) \t: Teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks.\n",
            "9. Doc 5 \t (Similarity: 0.1069) \t: Kecerdasan buatan digunakan untuk meningkatkan pelayanan kepada pelanggan.\n",
            "10. Doc 46 \t (Similarity: 0.0958) \t: Kerjasama antara pusat dan daerah sangat penting dalam pemerintahan.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung Logarithmic TF\n",
        "def log_tf_with_lemmatization(documents):\n",
        "    # Preprocessing dengan lemmatization\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    # Buat CountVectorizer untuk menghitung log TF\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents).toarray()\n",
        "    # Terapkan log untuk setiap elemen dari TF matrix\n",
        "    log_tf_matrix = np.log1p(tf_matrix)  # log(1 + tf)\n",
        "    return log_tf_matrix, vectorizer\n",
        "\n",
        "# Fungsi LSI dengan log TF\n",
        "def lsi_with_log_tf_lemmatization(documents, query, random_state=42):\n",
        "    # Buat TF biasa dengan lemmatization\n",
        "    tf_matrix, vectorizer = log_tf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (Log TF):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian\n",
        "lsi_with_log_tf_lemmatization(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWBJwOu7Jf9"
      },
      "source": [
        "**Double Normalization 0.5 Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FapLBiD7HyA",
        "outputId": "74722919-5c86-4d09-ee06-2b4bee3982c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (augmented TF):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Similarity: 0.3813) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 22 \t (Similarity: 0.3596) \t: Sekolah dasar memiliki peranan penting dalam pengembangan karakter anak.\n",
            "3. Doc 24 \t (Similarity: 0.3258) \t: Pendidikan inklusif memberikan kesempatan kepada semua anak untuk belajar.\n",
            "4. Doc 14 \t (Similarity: 0.3215) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "5. Doc 17 \t (Similarity: 0.2956) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "6. Doc 49 \t (Similarity: 0.2920) \t: Peran aktif masyarakat dalam mengawasi pemerintah sangat dibutuhkan.\n",
            "7. Doc 50 \t (Similarity: 0.2914) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "8. Doc 39 \t (Similarity: 0.2891) \t: Teknologi medis canggih membantu dalam diagnosis dan perawatan penyakit kompleks.\n",
            "9. Doc 28 \t (Similarity: 0.2829) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n",
            "10. Doc 45 \t (Similarity: 0.2822) \t: Anggaran negara harus dikelola dengan akuntabilitas yang tinggi.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung Augmented TF\n",
        "def augmented_tf_with_lemmatization(documents):\n",
        "    # Preprocessing dengan lemmatization\n",
        "    preprocessed_documents = [preprocess_with_lemmatization(doc) for doc in documents]\n",
        "    # Buat CountVectorizer untuk menghitung augmented TF\n",
        "    vectorizer = CountVectorizer()\n",
        "    tf_matrix = vectorizer.fit_transform(preprocessed_documents).toarray()\n",
        "    # Terapkan augmented TF untuk setiap dokumen\n",
        "    augmented_tf_matrix = 0.5 + 0.5 * (tf_matrix / tf_matrix.max(axis=1, keepdims=True))\n",
        "    return augmented_tf_matrix, vectorizer\n",
        "\n",
        "# Fungsi LSI dengan augmented TF\n",
        "def lsi_with_augmented_tf_lemmatization(documents, query, random_state=42):\n",
        "    # Buat augmented TF dengan lemmatization\n",
        "    tf_matrix, vectorizer = augmented_tf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=20, random_state=random_state)\n",
        "    lsi_matrix = svd.fit_transform(tf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung kesamaan cosine antara query dan dokumen\n",
        "    similarities = cosine_similarity(query_lsi, lsi_matrix)\n",
        "\n",
        "    # Urutkan dokumen berdasarkan kesamaan\n",
        "    ranked_docs = np.argsort(similarities[0])[::-1]\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (augmented TF):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "        print(f\"{i + 1}. Doc {idx+1} \\t (Similarity: {similarities[0][idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian\n",
        "lsi_with_augmented_tf_lemmatization(documents, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7EarGNB2iH0"
      },
      "source": [
        "# SKENARIO 4: EUCLIDEAN DISTANCE\n",
        "\n",
        "\n",
        "> Hipotesis: Cosine Similarity lebih baik untuk menghitung kemiripan teks karena lebih sensitif terhadap arah vektor daripada panjang vektor, yang bisa lebih relevan untuk teks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDrglp56zGk2",
        "outputId": "782ac6a0-46b9-474a-862c-bab0d2720676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pencarian dengan LSI (Lemmatization, Euclidean Distance):\n",
            "Query: Asupan nutrisi apa saja yang dibutuhkan untuk pertumbuhan anak?\n",
            "\n",
            "Top Documents:\n",
            "1. Doc 33 \t (Euclidean Distance: 0.1870) \t: Asupan nutrisi yang seimbang sangat penting untuk pertumbuhan anak.\n",
            "2. Doc 14 \t (Euclidean Distance: 0.4041) \t: Tanaman padi memerlukan air yang cukup untuk tumbuh dengan baik.\n",
            "3. Doc 17 \t (Euclidean Distance: 0.4695) \t: Budidaya tanaman hortikultura memberikan peluang ekonomi yang baik.\n",
            "4. Doc 50 \t (Euclidean Distance: 0.5280) \t: Kebijakan luar negeri harus selaras dengan kepentingan nasional.\n",
            "5. Doc 18 \t (Euclidean Distance: 0.6450) \t: Diversifikasi tanaman penting untuk mengurangi risiko gagal panen.\n",
            "6. Doc 1 \t (Euclidean Distance: 0.8348) \t: Teknologi semakin berkembang dan digunakan dalam banyak bidang.\n",
            "7. Doc 15 \t (Euclidean Distance: 0.8545) \t: Penggunaan pestisida harus dilakukan dengan hati-hati untuk menghindari dampak lingkungan.\n",
            "8. Doc 45 \t (Euclidean Distance: 0.8920) \t: Anggaran negara harus dikelola dengan akuntabilitas yang tinggi.\n",
            "9. Doc 37 \t (Euclidean Distance: 0.9884) \t: Masalah gizi buruk masih menjadi tantangan besar di beberapa daerah.\n",
            "10. Doc 28 \t (Euclidean Distance: 1.0962) \t: Pembelajaran jarak jauh menjadi alternatif di masa pandemi.\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk LSI dengan Lemmatization dan Euclidean Distance\n",
        "def lsi_with_lemmatization_euclidean(documents, query, random_state=42):\n",
        "    # Buat TF-IDF dengan lemmatization\n",
        "    tfidf_matrix, vectorizer = tf_with_lemmatization(documents)\n",
        "\n",
        "    # Lakukan Truncated SVD (LSI)\n",
        "    svd = TruncatedSVD(n_components=10, random_state=random_state)  # Gunakan lebih banyak komponen jika dibutuhkan\n",
        "    lsi_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Preprocessing query dengan lemmatization\n",
        "    preprocessed_query = preprocess_with_lemmatization(query)\n",
        "    query_vec = vectorizer.transform([preprocessed_query])\n",
        "    query_lsi = svd.transform(query_vec)\n",
        "\n",
        "    # Hitung Euclidean Distance antara query dan dokumen\n",
        "    distances = [euclidean(query_lsi[0], doc_vec) for doc_vec in lsi_matrix]\n",
        "\n",
        "    # Urutkan dokumen berdasarkan jarak terdekat (semakin kecil semakin mirip)\n",
        "    ranked_docs = np.argsort(distances)\n",
        "\n",
        "    # Ambil top-10 dokumen\n",
        "    top_10_docs = ranked_docs[:10]\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    print(\"Pencarian dengan LSI (Lemmatization, Euclidean Distance):\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop Documents:\")\n",
        "    for i, idx in enumerate(top_10_docs):\n",
        "      print(f\"{i + 1}. Doc {idx+1} \\t (Euclidean Distance: {distances[idx]:.4f}) \\t: {documents[idx]}\")\n",
        "\n",
        "# Lakukan pencarian dengan lemmatization menggunakan Euclidean Distance\n",
        "lsi_with_lemmatization_euclidean(documents, query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d64b4c8af264665a0d69ef8417ec472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ecb7f91fd8e4494baa8748f84f08b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0edbf3e4095a4745a8c00ac2ebf9a999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdaddb290d5f41e788adaa7f22e0128f",
            "placeholder": "​",
            "style": "IPY_MODEL_18e0f490f48e4ae0af403f87db9ab8f7",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: "
          }
        },
        "18e0f490f48e4ae0af403f87db9ab8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19052620a0e4468aaad9cb876cb3ca46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "283ce6cf234e486b9c07e36bcfa6828c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0edbf3e4095a4745a8c00ac2ebf9a999",
              "IPY_MODEL_b6142e1f524b4b5b9272343b87d41b62",
              "IPY_MODEL_8096f85097364e7b821b1950ee5752f3"
            ],
            "layout": "IPY_MODEL_9ed108d1645546cc988f35b63f684a81"
          }
        },
        "549da9acc6c94a8b95601c44fcbb1e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59fccf02d34e463582105f5a13579e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ecb7f91fd8e4494baa8748f84f08b2c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f0fc3f9970f446988fe55997d3e98a0",
            "value": " 392k/? [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "69571456741d4cb0a9bbffd45dfa25cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea789815d7fa42bcbac10424581e0d75",
              "IPY_MODEL_a12eeac1fd38438b8e02550197ec6079",
              "IPY_MODEL_59fccf02d34e463582105f5a13579e8e"
            ],
            "layout": "IPY_MODEL_ea1fe098989f488f9c8b61ac28fe043f"
          }
        },
        "7f0fc3f9970f446988fe55997d3e98a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8096f85097364e7b821b1950ee5752f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4eb820290e34b3bada4d81ceccd7532",
            "placeholder": "​",
            "style": "IPY_MODEL_f1c4601aa59944c69c3072d09342c7a5",
            "value": " 392k/? [00:00&lt;00:00, 17.2MB/s]"
          }
        },
        "95652cd25d494439bf5ac92a3cb98425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ed108d1645546cc988f35b63f684a81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a12eeac1fd38438b8e02550197ec6079": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc84d2a4f8a540aca0992bfc481efe84",
            "max": 48453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19052620a0e4468aaad9cb876cb3ca46",
            "value": 48453
          }
        },
        "b6142e1f524b4b5b9272343b87d41b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed40b324ca834ee1b5d9a1d5260c5b6b",
            "max": 48453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d64b4c8af264665a0d69ef8417ec472",
            "value": 48453
          }
        },
        "cdaddb290d5f41e788adaa7f22e0128f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4eb820290e34b3bada4d81ceccd7532": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc84d2a4f8a540aca0992bfc481efe84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1fe098989f488f9c8b61ac28fe043f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea789815d7fa42bcbac10424581e0d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549da9acc6c94a8b95601c44fcbb1e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_95652cd25d494439bf5ac92a3cb98425",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: "
          }
        },
        "ed40b324ca834ee1b5d9a1d5260c5b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c4601aa59944c69c3072d09342c7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
